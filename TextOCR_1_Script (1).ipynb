{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c2d72b-4eff-425c-a78c-7d98eabd7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from ultralytics import YOLO\n",
    "#import easyocr\n",
    "import logging\n",
    "import pytesseract\n",
    "import editdistance\n",
    "from tabulate import tabulate\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ae007d-7e49-4158-970d-f3bbe3f48d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sub_annot.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m filtered_csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannot.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path for the new filtered CSV\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the original annotation CSV file\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m annot_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_csv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Get the list of image files currently present in the folder\u001b[39;00m\n\u001b[0;32m     13\u001b[0m image_files_in_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(image_folder))\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\dml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\dml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\dml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\dml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\dml\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sub_annot.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "image_folder = 'trained_images'  # Folder where you uploaded images\n",
    "original_csv_file = 'sub_annot.csv'  # Original annot.csv file\n",
    "filtered_csv_file = 'annot.csv'  # Path for the new filtered CSV\n",
    "\n",
    "# Load the original annotation CSV file\n",
    "annot_df = pd.read_csv(original_csv_file)\n",
    "\n",
    "# Get the list of image files currently present in the folder\n",
    "image_files_in_folder = set(os.listdir(image_folder))\n",
    "\n",
    "# Filter annotations based on available images in the folder\n",
    "available_image_ids = set([os.path.splitext(img)[0] for img in image_files_in_folder if img.endswith(('.jpg', '.png'))])\n",
    "\n",
    "# Filter the annotations DataFrame to only include available image IDs\n",
    "filtered_annot_df = annot_df[annot_df['image_id'].isin(available_image_ids)]\n",
    "\n",
    "# Save the filtered annotations to a new CSV file\n",
    "filtered_annot_df.to_csv(filtered_csv_file, index=False)\n",
    "\n",
    "# Print some stats and a success message\n",
    "print(f\"Total images in the folder: {len(available_image_ids)}\")\n",
    "print(f\"Total images with annotations: {len(filtered_annot_df['image_id'].unique())}\")\n",
    "print(f\"Filtered annotations saved to: {filtered_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c08dd3f4-7464-4d2a-a869-690d73f557bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created at dataset/images and dataset/labels\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "DATA_DIR = 'trained_images'\n",
    "ANNOT_PATH = 'annot.csv'\n",
    "IMAGE_OUTPUT_DIR = 'dataset/images'\n",
    "LABEL_OUTPUT_DIR = 'dataset/labels'\n",
    "SUBSET_RATIO = 0.8\n",
    "\n",
    "# Prepare directories\n",
    "def prepare_directories(image_output_dir, label_output_dir):\n",
    "    os.makedirs(image_output_dir, exist_ok=True)\n",
    "    os.makedirs(label_output_dir, exist_ok=True)\n",
    "    print(f\"Directories created at {image_output_dir} and {label_output_dir}\")\n",
    "\n",
    "# Create directories for images and labels\n",
    "prepare_directories(IMAGE_OUTPUT_DIR, LABEL_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33cb3d70-6b5a-404a-ae3b-99b59a00f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split and reduce dataset size\n",
    "def reduce_dataset_size(image_ids, subset_ratio=0.02):\n",
    "    train_ids, test_ids = train_test_split(image_ids, test_size=0.03, random_state=42)\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=0.2, random_state=42)\n",
    "    small_train_ids, _ = train_test_split(train_ids, test_size=1-subset_ratio, random_state=42)\n",
    "    small_val_ids, _ = train_test_split(val_ids, test_size=1-subset_ratio, random_state=42)\n",
    "    small_test_ids, _ = train_test_split(test_ids, test_size=1-subset_ratio, random_state=42)\n",
    "    return small_train_ids, small_val_ids, small_test_ids\n",
    "\n",
    "# Load and split data\n",
    "annot_df = pd.read_csv(ANNOT_PATH)\n",
    "image_ids = annot_df['image_id'].unique()\n",
    "small_train_ids, small_val_ids, small_test_ids = reduce_dataset_size(image_ids, SUBSET_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0407d1-92ed-45c4-84c8-c0ce0f4052e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert bounding boxes to YOLO format\n",
    "def convert_to_yolo_format(size, box):\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = (box[0] + box[2]) / 2.0\n",
    "    y = (box[1] + box[3]) / 2.0\n",
    "    w = box[2] - box[0]\n",
    "    h = box[3] - box[1]\n",
    "    x = max(0, min(1, x * dw))\n",
    "    y = max(0, min(1, y * dh))\n",
    "    w = max(0, min(1, w * dw))\n",
    "    h = max(0, min(1, h * dh))\n",
    "    return (x, y, w, h)\n",
    "    \n",
    "# Copy images and create label files in YOLO format\n",
    "def process_images_and_labels(image_ids, annot_df):\n",
    "    for image_id in image_ids:\n",
    "        img_path = os.path.join(DATA_DIR, f'{image_id}.jpg')\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w, _ = img.shape\n",
    "        \n",
    "        new_img_path = os.path.join(IMAGE_OUTPUT_DIR, f'{image_id}.jpg')\n",
    "        cv2.imwrite(new_img_path, img)\n",
    "        \n",
    "        records = annot_df[annot_df['image_id'] == image_id]\n",
    "        boxes = records['bbox'].apply(json.loads).tolist()\n",
    "        boxes = [[b[0], b[1], b[0] + b[2], b[1] + b[3]] for b in boxes]\n",
    "        \n",
    "        label_file_path = os.path.join(LABEL_OUTPUT_DIR, f'{image_id}.txt')\n",
    "        with open(label_file_path, 'w') as f:\n",
    "            for box in boxes:\n",
    "                yolo_box = convert_to_yolo_format((w, h), box)\n",
    "                f.write(f'0 {yolo_box[0]} {yolo_box[1]} {yolo_box[2]} {yolo_box[3]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cda5d2-0e7e-443e-b476-c8e9cd4adc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13810ba2-728b-4842-aad0-727bfbd90d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    }
   ],
   "source": [
    "# Process images and labels, only run when the dataset folder is not already in place.\n",
    "#process_images_and_labels(image_ids, annot_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1a2da4-649c-4c29-a50f-6ddb4c930186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the text files listing paths to images\n",
    "def create_text_file(image_ids, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for image_id in image_ids:\n",
    "            img_path = os.path.join(os.getcwd(), IMAGE_OUTPUT_DIR, f'{image_id}.jpg')\n",
    "            f.write(f'{img_path}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58acff2c-1b6e-4ff1-92b0-f85d5aa0667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_text_file(small_train_ids, 'dataset/train.txt')\n",
    "create_text_file(small_val_ids, 'dataset/val.txt')\n",
    "create_text_file(small_test_ids, 'dataset/test.txt')\n",
    "\n",
    "print(\"Text files created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d78c9ddf-748f-49f0-8b89-80c45047f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextOCRDataset(Dataset):\n",
    "    def __init__(self, image_ids, annot_df, data_dir, transforms=None, target_size=(640, 640)):\n",
    "        self.image_ids = image_ids\n",
    "        self.annot_df = annot_df\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.data_dir, f'{image_id}.jpg')\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        original_width, original_height = img.size\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        # Rescale bounding boxes to match the resized image dimensions\n",
    "        records = self.annot_df[self.annot_df['image_id'] == image_id]\n",
    "        boxes = records['bbox'].apply(json.loads).tolist()\n",
    "        boxes = [[b[0] * self.target_size[0] / original_width, \n",
    "                  b[1] * self.target_size[1] / original_height,\n",
    "                  (b[0] + b[2]) * self.target_size[0] / original_width,\n",
    "                  (b[1] + b[3]) * self.target_size[1] / original_height] for b in boxes]\n",
    "\n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.ones((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "# Data Transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize((640, 640)),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e42e221-30e7-4555-b2d9-140c98152412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "small_train_dataset = TextOCRDataset(small_train_ids, annot_df, DATA_DIR, transforms=transform)\n",
    "val_dataset = TextOCRDataset(small_val_ids, annot_df, DATA_DIR, transforms=transform)\n",
    "test_dataset = TextOCRDataset(small_test_ids, annot_df, DATA_DIR, transforms=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "small_train_loader = DataLoader(small_train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc43fbcb-3e8d-41da-8e1e-e33a3e93c417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1892\n",
      "Validation dataset size: 473\n",
      "Test dataset size: 73\n"
     ]
    }
   ],
   "source": [
    "# Print dataset sizes to verify reduction\n",
    "print(f\"Training dataset size: {len(small_train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5150c892-4f13-4d58-9a4f-7e9bb0a3b409",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python integer -14 out of bounds for uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Visualize some training data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mvisualize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmall_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m visualize_data(small_train_dataset, idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mvisualize_data\u001b[0;34m(dataset, idx, color, thickness, figsize)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_data\u001b[39m(dataset, idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, color\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m)):\n\u001b[0;32m----> 2\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m     img \u001b[38;5;241m=\u001b[39m (img\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mTextOCRDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Apply transformations\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 17\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Rescale bounding boxes to match the resized image dimensions\u001b[39;00m\n\u001b[1;32m     20\u001b[0m records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannot_df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannot_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m image_id]\n",
      "File \u001b[0;32m~/conda/envs/dml/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/conda/envs/dml/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/dml/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/dml/lib/python3.11/site-packages/torchvision/transforms/transforms.py:1280\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1280\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/conda/envs/dml/lib/python3.11/site-packages/torchvision/transforms/functional.py:959\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    957\u001b[0m     _log_api_usage_once(adjust_hue)\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n",
      "File \u001b[0;32m~/conda/envs/dml/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py:114\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# uint8 addition take cares of rotation across boundaries\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 114\u001b[0m     np_h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m h \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np_h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHSV\u001b[39m\u001b[38;5;124m\"\u001b[39m, (h, s, v))\u001b[38;5;241m.\u001b[39mconvert(input_mode)\n",
      "\u001b[0;31mOverflowError\u001b[0m: Python integer -14 out of bounds for uint8"
     ]
    }
   ],
   "source": [
    "def visualize_data(dataset, idx=0, color=(0, 255, 0), thickness=2, figsize=(10, 10)):\n",
    "    img, target = dataset[idx]\n",
    "    boxes = target['boxes'].cpu().numpy()\n",
    "    img = (img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "    # Display the image with bounding boxes using matplotlib\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some training data\n",
    "visualize_data(small_train_dataset, idx=0)\n",
    "visualize_data(small_train_dataset, idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72f3f020-45f0-4e69-971a-26be94825767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e087905-7419-43af-bc11-84c43247fc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.23 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.20 🚀 Python-3.11.10 torch-2.3.1+cu118 CPU (Intel Xeon 2.00GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov3u.pt, data=config.yaml, epochs=10, time=None, patience=100, batch=5, imgsz=320, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 1]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     37056  ultralytics.nn.modules.block.Bottleneck      [64, 64]                      \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    295680  ultralytics.nn.modules.block.Bottleneck      [128, 128]                    \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  8   4724736  ultralytics.nn.modules.block.Bottleneck      [256, 256]                    \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  8  18886656  ultralytics.nn.modules.block.Bottleneck      [512, 512]                    \n",
      "  9                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 2]             \n",
      " 10                  -1  4  37761024  ultralytics.nn.modules.block.Bottleneck      [1024, 1024]                  \n",
      " 11                  -1  1   9440256  ultralytics.nn.modules.block.Bottleneck      [1024, 1024, False]           \n",
      " 12                  -1  1    525312  ultralytics.nn.modules.conv.Conv             [1024, 512, 1, 1]             \n",
      " 13                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 1]             \n",
      " 14                  -1  1    525312  ultralytics.nn.modules.conv.Conv             [1024, 512, 1, 1]             \n",
      " 15                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 1]             \n",
      " 16                  -2  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 18             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   2950656  ultralytics.nn.modules.block.Bottleneck      [768, 512, False]             \n",
      " 20                  -1  1   2360832  ultralytics.nn.modules.block.Bottleneck      [512, 512, False]             \n",
      " 21                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 22                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 1]              \n",
      " 23                  -2  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 24                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 25             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 26                  -1  1    738048  ultralytics.nn.modules.block.Bottleneck      [384, 256, False]             \n",
      " 27                  -1  2   1181184  ultralytics.nn.modules.block.Bottleneck      [256, 256, False]             \n",
      " 28        [27, 22, 15]  1   7058131  ultralytics.nn.modules.head.Detect           [1, [256, 512, 1024]]         \n",
      "YOLOv3 summary: 310 layers, 103,693,235 parameters, 103,693,219 gradients, 283.0 GFLOPs\n",
      "\n",
      "Transferred 511/517 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.28.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/student/dataset/labels... 1892 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1892/1892 \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/student/dataset/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/student/dataset/labels... 473 images, 0 backgrounds, 0 corrupt: 100%|██████████| 473/473 [00:0\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/student/dataset/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 84 weight(decay=0.0), 91 weight(decay=0.0005078125), 90 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 320 train, 320 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      2.637      1.898      1.392         44        320: 100%|██████████| 379/379 [21:51<00:\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174      0.255      0.136      0.101     0.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G       2.65      1.766      1.381         96        320: 100%|██████████| 379/379 [21:15<00:\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174      0.107      0.151     0.0345     0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      2.578      1.696      1.345         54        320: 100%|██████████| 379/379 [21:04<00:\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174      0.295      0.168      0.123      0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      2.469      1.612      1.307         70        320: 100%|██████████| 379/379 [20:57<00:\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174      0.339      0.178      0.155     0.0611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G       2.36      1.492      1.265         28        320: 100%|██████████| 379/379 [20:53<00:\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174       0.41       0.21      0.195     0.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      2.269      1.412      1.225        188        320: 100%|██████████| 379/379 [21:21<00:\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 [01:30<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174      0.422      0.237       0.22     0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      2.154      1.342      1.192        107        320: 100%|██████████| 379/379 [21:34<00:00,  3.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 [01:29<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174      0.433      0.244      0.224      0.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      2.117      1.293      1.173         27        320: 100%|██████████| 379/379 [21:28<00:\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174      0.446      0.255      0.233      0.106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      2.029      1.237      1.147         48        320: 100%|██████████| 379/379 [21:27<00:\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 48/48 ["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        473      20174      0.481      0.262      0.254      0.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.966      1.184      1.138         85        320:  66%|██████▌   | 249/379 [14:17<07:"
     ]
    }
   ],
   "source": [
    "# Train YOLOv3\n",
    "\n",
    "model = YOLO('yolov3u.pt')\n",
    "model.train(data='config.yaml', epochs=10, imgsz=320, batch=5, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e2ae518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 256x320 14 texts, 292.9ms\n",
      "Speed: 0.0ms preprocess, 292.9ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 320)\n",
      "\n",
      "Extracted Texts:\n",
      "+----------------+---------------------+\n",
      "| Extracted Text |     Confidence      |\n",
      "+----------------+---------------------+\n",
      "|      NEWS      | 0.7008996605873108  |\n",
      "|    WERLD'S     | 0.6780672669410706  |\n",
      "|                | 0.6735836267471313  |\n",
      "|      woop      | 0.6520718336105347  |\n",
      "|     SATUF      | 0.6499629616737366  |\n",
      "|     GREEN.     | 0.5873026251792908  |\n",
      "|                | 0.5679686069488525  |\n",
      "|    Evening     | 0.5316756367683411  |\n",
      "|       UU       | 0.4927869737148285  |\n",
      "|     AORST      | 0.47362199425697327 |\n",
      "|    HOCKER’     | 0.44166478514671326 |\n",
      "| om A Se eee =  | 0.4334736466407776  |\n",
      "|                | 0.3711823523044586  |\n",
      "|     Con Og     | 0.26255112886428833 |\n",
      "|  ~—caariisoy   |                     |\n",
      "+----------------+---------------------+\n",
      "\n",
      "Character Error Rate (CER): 3.07\n",
      "Word Error Rate (WER): 3.40\n"
     ]
    }
   ],
   "source": [
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "\n",
    "# Load the trained model with the best weights\n",
    "model = YOLO('C:\\Project\\\\vscode\\SSY340-text-extraction-project/best.pt')\n",
    "\n",
    "# Define the function to draw bounding boxes and labels on the image\n",
    "def draw_boxes(image, boxes, confidences, class_ids, class_names):\n",
    "    extracted_texts = []\n",
    "    for box, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "        \n",
    "        # Extract the region of interest (ROI) for OCR\n",
    "        roi = image[y1:y2, x1:x2]\n",
    "        \n",
    "        # Use Tesseract to extract text from the ROI\n",
    "        extracted_text = pytesseract.image_to_string(roi, config='--psm 6').strip()  # Adjust psm mode as needed\n",
    "        \n",
    "        label = f\"{extracted_text} {confidence:.2f}\"\n",
    "        extracted_texts.append((extracted_text, confidence))\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the label\n",
    "        font_scale = 0.5\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        (text_width, text_height) = cv2.getTextSize(label, font, fontScale=font_scale, thickness=1)[0]\n",
    "        text_offset_x = x1\n",
    "        text_offset_y = y1 - 5\n",
    "        box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height - 2))\n",
    "        cv2.rectangle(image, box_coords[0], box_coords[1], (0, 255, 0), cv2.FILLED)\n",
    "        cv2.putText(image, label, (text_offset_x, text_offset_y), font, fontScale=font_scale, color=(0, 0, 0), thickness=1)\n",
    "\n",
    "    return image, extracted_texts\n",
    "\n",
    "# Define the function to calculate CER and WER\n",
    "def calculate_error_rates(reference, hypothesis):\n",
    "    cer = editdistance.eval(reference, hypothesis) / len(reference) if len(reference) > 0 else 0\n",
    "    wer = editdistance.eval(reference.split(), hypothesis.split()) / len(reference.split()) if len(reference.split()) > 0 else 0\n",
    "    return cer, wer\n",
    "\n",
    "# Define the function to perform inference, extract text, and calculate error rates\n",
    "def infer_and_show(model, image_path, reference_text):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    assert image is not None, f\"Error: Unable to load image at {image_path}\"\n",
    "\n",
    "    # Perform inference\n",
    "    results = model(image)\n",
    "    \n",
    "    # Extract the results\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()  # Bounding boxes in xyxy format\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()  # Confidence scores\n",
    "    class_ids = results[0].boxes.cls.cpu().numpy()  # Class IDs\n",
    "    class_names = results[0].names  # Class names\n",
    "\n",
    "    # Draw bounding boxes and labels on the image, and extract texts\n",
    "    annotated_image, extracted_texts = draw_boxes(image, boxes, confidences, class_ids, class_names)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Inference', annotated_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Print the extracted texts in a table format\n",
    "    print(\"\\nExtracted Texts:\")\n",
    "    headers = [\"Extracted Text\", \"Confidence\"]\n",
    "    print(tabulate(extracted_texts, headers=headers, tablefmt=\"pretty\"))\n",
    "\n",
    "    # Calculate CER and WER\n",
    "    extracted_text_combined = \" \".join(text[0] for text in extracted_texts)\n",
    "    cer, wer = calculate_error_rates(reference_text, extracted_text_combined)\n",
    "    print(f\"\\nCharacter Error Rate (CER): {cer:.2f}\")\n",
    "    print(f\"Word Error Rate (WER): {wer:.2f}\")\n",
    "\n",
    "# Path to the image you want to perform inference on\n",
    "#image_path = 'TextOCR/train_val_images/train_images/0af5d7dc935d0563.jpg'\n",
    "image_path = 'TextOCR/train_val_images/train_images/0af5d7dc935d0563.jpg'\n",
    "# Reference text for calculating CER and WER (this should be the ground truth text for the image)\n",
    "reference_text = \"your ground truth text here\"\n",
    "\n",
    "# Perform inference, extract text, and calculate error rates\n",
    "infer_and_show(model, image_path, reference_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9418ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_test_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tabulate(results, headers\u001b[38;5;241m=\u001b[39mheaders, tablefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Process the test dataset and calculate error rates\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m process_test_dataset(model, \u001b[43msmall_test_ids\u001b[49m, annot_df, num_samples\u001b[38;5;241m=\u001b[39mNUM_SAMPLES)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'small_test_ids' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 2\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "\n",
    "# Load the trained model with the best weights\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "# Define the function to draw bounding boxes and labels on the image\n",
    "def draw_boxes(image, boxes, confidences, class_ids, class_names):\n",
    "    extracted_texts = []\n",
    "    for box, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "        \n",
    "        # Extract the region of interest (ROI) for OCR\n",
    "        roi = image[y1:y2, x1:x2]\n",
    "        \n",
    "        # Use Tesseract to extract text from the ROI\n",
    "        extracted_text = pytesseract.image_to_string(roi, config='--psm 6').strip()  # Adjust psm mode as needed\n",
    "        \n",
    "        label = f\"{extracted_text} {confidence:.2f}\"\n",
    "        extracted_texts.append(extracted_text)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the label\n",
    "        font_scale = 0.5\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        (text_width, text_height) = cv2.getTextSize(label, font, fontScale=font_scale, thickness=1)[0]\n",
    "        text_offset_x = x1\n",
    "        text_offset_y = y1 - 5\n",
    "        box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height - 2))\n",
    "        cv2.rectangle(image, box_coords[0], box_coords[1], (0, 255, 0), cv2.FILLED)\n",
    "        cv2.putText(image, label, (text_offset_x, text_offset_y), font, fontScale=font_scale, color=(0, 0, 0), thickness=1)\n",
    "\n",
    "    return image, extracted_texts\n",
    "\n",
    "# Define the function to calculate CER and WER\n",
    "def calculate_error_rates(reference, hypothesis):\n",
    "    cer = editdistance.eval(reference, hypothesis) / len(reference) if len(reference) > 0 else 0\n",
    "    wer = editdistance.eval(reference.split(), hypothesis.split()) / len(reference.split()) if len(reference.split()) > 0 else 0\n",
    "    return cer, wer\n",
    "\n",
    "# Define the function to perform inference, extract text, and calculate error rates for a single image\n",
    "def infer_and_show(model, image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    assert image is not None, f\"Error: Unable to load image at {image_path}\"\n",
    "\n",
    "    # Perform inference\n",
    "    results = model(image)\n",
    "    \n",
    "    # Extract the results\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()  # Bounding boxes in xyxy format\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()  # Confidence scores\n",
    "    class_ids = results[0].boxes.cls.cpu().numpy()  # Class IDs\n",
    "    class_names = results[0].names  # Class names\n",
    "\n",
    "    # Draw bounding boxes and labels on the image, and extract texts\n",
    "    annotated_image, extracted_texts = draw_boxes(image, boxes, confidences, class_ids, class_names)\n",
    "\n",
    "    # Combine extracted texts\n",
    "    extracted_text_combined = \" \".join(extracted_texts)\n",
    "\n",
    "    return annotated_image, extracted_text_combined\n",
    "\n",
    "# Define the function to process the test dataset\n",
    "def process_test_dataset(model, test_ids, annot_df, num_samples=3):\n",
    "    # Initialize lists to store CER and WER for each image\n",
    "    cer_list = []\n",
    "    wer_list = []\n",
    "    images_to_display = []\n",
    "\n",
    "    # Randomly select sample images for display\n",
    "    sample_ids = random.sample(list(test_ids), num_samples)\n",
    "\n",
    "    # Process each image in the test dataset\n",
    "    for image_id in test_ids:\n",
    "        img_path = os.path.join(DATA_DIR, f'{image_id}.jpg')\n",
    "        reference_text = \" \".join(annot_df[annot_df['image_id'] == image_id]['utf8_string'].tolist())\n",
    "\n",
    "        # Perform inference and extract text\n",
    "        annotated_image, extracted_text = infer_and_show(model, img_path)\n",
    "        \n",
    "        # Calculate CER and WER\n",
    "        cer, wer = calculate_error_rates(reference_text, extracted_text)\n",
    "        cer_list.append(cer)\n",
    "        wer_list.append(wer)\n",
    "        \n",
    "        # Store the image to display later if it's in the sample_ids\n",
    "        if image_id in sample_ids:\n",
    "            images_to_display.append((annotated_image, image_id))\n",
    "\n",
    "        # Print the extracted text and error rates for each image\n",
    "        print(f\"\\nImage: {image_id}\")\n",
    "        print(f\"Reference Text: {reference_text}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(f\"Character Error Rate (CER): {cer:.2f}\")\n",
    "        print(f\"Word Error Rate (WER): {wer:.2f}\")\n",
    "\t\t    # Display the sample images\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "    for ax, (image, image_id) in zip(axes, images_to_display):\n",
    "        ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        ax.set_title(f\"Image ID: {image_id}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and print overall CER and WER\n",
    "    overall_cer = sum(cer_list) / len(cer_list) if cer_list else 0\n",
    "    overall_wer = sum(wer_list) / len(wer_list) if wer_list else 0\n",
    "\n",
    "    # Print the final results in a table format\n",
    "    headers = [\"Metric\", \"Value\"]\n",
    "    results = [\n",
    "        [\"Overall Character Error Rate (CER)\", f\"{overall_cer:.2f}\"],\n",
    "        [\"Overall Word Error Rate (WER)\", f\"{overall_wer:.2f}\"]\n",
    "    ]\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(tabulate(results, headers=headers, tablefmt=\"pretty\"))\n",
    "\n",
    "# Process the test dataset and calculate error rates\n",
    "process_test_dataset(model, small_test_ids, annot_df, num_samples=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e6ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a79397c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Set the Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "model1 = YOLO('runs/detect/train2/weights/best.pt')\n",
    "\n",
    "def recognize_text(model, dataset, conf_threshold=0.25):\n",
    "    detected_texts = []  # Initialize list to store detected texts\n",
    "    \n",
    "    for img_index, (img, _) in enumerate(dataset):\n",
    "        # Convert the tensor to a numpy array if necessary\n",
    "        img_np = img.permute(1, 2, 0).numpy()  # Assuming img is a PyTorch tensor\n",
    "        logging.info(f\"Processing image {img_index} with shape: {img_np.shape}\")\n",
    "        \n",
    "        # Perform detection using the model\n",
    "        results = model(img_np)  # Call the model directly\n",
    "        \n",
    "        # Log the results\n",
    "        logging.info(f\"Image {img_index} results: {results}\")\n",
    "\n",
    "        # Check if there are detections\n",
    "        if not results or len(results[0].boxes.xyxy) == 0:\n",
    "            logging.warning(f\"No detections for image {img_index}\")\n",
    "            detected_texts.append([])  # Append empty list for this image\n",
    "            continue  # Skip to the next image\n",
    "\n",
    "        # Parse the results\n",
    "        bboxes = results[0].boxes.xyxy.numpy()  # Get bounding boxes (x1, y1, x2, y2)\n",
    "        logging.info(f\"Detected bounding boxes for image {img_index}: {bboxes}\")\n",
    "\n",
    "        texts = []\n",
    "        for bbox in bboxes:  # Iterate through each detected bounding box\n",
    "            text = extract_text_from_bbox(img_np, bbox)\n",
    "            if text:  # Only append if text is not empty\n",
    "                texts.append(text)\n",
    "        \n",
    "        detected_texts.append(texts)\n",
    "    \n",
    "    return detected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6385c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
